{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# Scaling the range of the image to [-1, 1]\n",
    "# Because we are using tanh as the activation function in the last layer of the generator\n",
    "# and tanh restricts the weights in the range [-1, 1]\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W', dtype=tf.float32, shape=shape, initializer=initer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b', dtype=tf.float32, initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_out: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape()[1]\n",
    "        W = weight_variable(shape=[in_dim, num_units])\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_units])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.matmul(x, W)\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PRelu (parametarized leaky relu to correctly avoid dying Relu problem )  and Leaky Relu problem where negetive data only provide small amount of slope for learning parameter so that network itself familirize the required slope paramter for negative input value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prelu(inpt, name):\n",
    "    \"\"\"\n",
    "    @inpt: input from previous layer\n",
    "    @name: layer name\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        i = int(inpt.get_shape()[-1])\n",
    "        alpha = tf.get_variable('alpha', shape=(i,))\n",
    "        output = tf.nn.relu(inpt) + tf.multiply(alpha, -tf.nn.relu(-inpt))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, num_filters, filter_size, stride, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a 2D convolution layer\n",
    "    :param x: input from previous layer\n",
    "    :param filter_size: size of each filter\n",
    "    :param num_filters: number of filters (or output feature maps)\n",
    "    :param stride: filter stride\n",
    "    :param name: layer name\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        num_in_channel = x.get_shape().as_list()[-1]\n",
    "        shape = [filter_size, filter_size, num_in_channel, num_filters]\n",
    "        W = weight_variable(shape=shape)\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_filters])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "#         Using non - linearity RELU\n",
    "            return tf.nn.relu(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    \"\"\"\n",
    "    Flattens the output of the convolutional layer to be fed into fully-connected layer\n",
    "    :param layer: input array\n",
    "    :return: flattened array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Flatten_layer'):\n",
    "        layer_shape = layer.get_shape()\n",
    "        num_features = layer_shape[1:4].num_elements()\n",
    "        layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, keep_prob,name):\n",
    "    \"\"\"Create a dropout layer.\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001  # The optimization initial learning rate\n",
    "epochs = 20  # Total number of training epochs\n",
    "batch_size = 50  # Training batch size\n",
    "display_freq = 50  # Frequency of displaying the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name='Y')\n",
    "noise = tf.placeholder(tf.float32,shape=[None, 64])\n",
    "phase = tf.placeholder(tf.bool, name='phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator network\n",
    "g_net = fc_layer(noise,1024,name='g_fc1',use_relu=False)\n",
    "g_net = Prelu(g_net, name='g_prelu_1')\n",
    "#     Post activation Batch Normalization \n",
    "g_net = layers.batch_norm(g_net, is_training=phase)\n",
    "\n",
    "g_net = fc_layer(g_net,7 * 7 * 256, name='g_fc2', use_relu=False)\n",
    "g_net = Prelu(g_net, name='g_prelu_2')\n",
    "#     Post activation Batch Normalization\n",
    "g_net = layers.batch_norm(g_net, is_training=phase)\n",
    "\n",
    "g_net = tf.reshape(g_net, [-1, 7, 7, 256])\n",
    "g_net = layers.conv2d_transpose(g_net, 64, [4, 4], stride=2)\n",
    "g_net = layers.conv2d_transpose(g_net, 32, [4, 4], stride=2)\n",
    "# Make sure that generator output is in the same range as `inputs`\n",
    "# ie [-1, 1].\n",
    "g_output = layers.conv2d(g_net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "d_net = conv_layer(x, 64, 5, stride = 2, name='d_conv_1',use_relu=False)\n",
    "d_net = Prelu(d_net, name='d_prelu_1')\n",
    "\n",
    "d_net = dropout(d_net, 0.3)\n",
    "\n",
    "d_net = conv_layer(d_net, 128, 5, stride = 2, name='d_conv_2',use_relu=False)\n",
    "d_net = Prelu(d_net, name='d_prelu_2')\n",
    "\n",
    "d_net = dropout(d_net, 0.3)\n",
    "\n",
    "d_net = flatten_layer(d_net)\n",
    "d_net = fc_layer(d_net,1,name='output_discriminator',use_relu=False)\n",
    "d_output= tf.nn.sigmoid(d_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('generator'):\n",
    "    with tf.variable_scope('Train'):\n",
    "        with tf.variable_scope('Loss'):\n",
    "            g_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=g_output), name='loss')\n",
    "        tf.summary.scalar('loss', g_loss)\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                g_optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam-op').minimize(g_loss)\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            g_correct_prediction = tf.equal(tf.argmax(g_output, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "            g_accuracy = tf.reduce_mean(tf.cast(g_correct_prediction, tf.float32), name='accuracy')\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        with tf.variable_scope('Prediction'):\n",
    "            g_cls_prediction = tf.argmax(g_output, axis=1, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('discriminator'):\n",
    "    with tf.variable_scope('Train'):\n",
    "        with tf.variable_scope('Loss'):\n",
    "            d_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=d_output), name='loss')\n",
    "        tf.summary.scalar('loss', d_loss)\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            d_optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam-op').minimize(d_loss)\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            d_correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "            d_accuracy = tf.reduce_mean(tf.cast(d_correct_prediction, tf.float32), name='accuracy')\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        with tf.variable_scope('Prediction'):\n",
    "            d_cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
