{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcuts for later.\n",
    "queues = tf.contrib.slim.queues\n",
    "layers = tf.contrib.layers\n",
    "ds = tf.contrib.distributions\n",
    "framework = tf.contrib.framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgan = tf.contrib.gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "  \n",
    "\n",
    "def visualize_training_generator(train_step_num, start_time, data_np):\n",
    "    \"\"\"Visualize generator outputs during training.\n",
    "    \n",
    "    Args:\n",
    "        train_step_num: The training step number. A python integer.\n",
    "        start_time: Time when training started. The output of `time.time()`. A\n",
    "            python float.\n",
    "        data: Data to plot. A numpy array, most likely from an evaluated TensorFlow\n",
    "            tensor.\n",
    "    \"\"\"\n",
    "    print('Training step: %i' % train_step_num)\n",
    "    time_since_start = (time.time() - start_time) / 60.0\n",
    "    print('Time since start: %f m' % time_since_start)\n",
    "    print('Steps per min: %f' % (train_step_num / time_since_start))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(data_np), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_digits(tensor_to_visualize):\n",
    "    \"\"\"Visualize an image once. Used to visualize generator before training.\n",
    "    \n",
    "    Args:\n",
    "        tensor_to_visualize: An image tensor to visualize. A python Tensor.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with queues.QueueRunners(sess):\n",
    "            images_np = sess.run(tensor_to_visualize)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(images_np), cmap='gray')\n",
    "\n",
    "def evaluate_tfgan_loss(gan_loss, name=None):\n",
    "    \"\"\"Evaluate GAN losses. Used to check that the graph is correct.\n",
    "    \n",
    "    Args:\n",
    "        gan_loss: A GANLoss tuple.\n",
    "        name: Optional. If present, append to debug output.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with queues.QueueRunners(sess):\n",
    "            gen_loss_np = sess.run(gan_loss.generator_loss)\n",
    "            dis_loss_np = sess.run(gan_loss.discriminator_loss)\n",
    "    if name:\n",
    "        print('%s generator loss: %f' % (name, gen_loss_np))\n",
    "        print('%s discriminator loss: %f'% (name, dis_loss_np))\n",
    "    else:\n",
    "        print('Generator loss: %f' % gen_loss_np)\n",
    "        print('Discriminator loss: %f'% dis_loss_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# Scaling the range of the image to [-1, 1]\n",
    "# Because we are using tanh as the activation function in the last layer of the generator\n",
    "# and tanh restricts the weights in the range [-1, 1]\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:batch_size].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(noise, weight_decay=2.5e-5, is_training=True):\n",
    "    \"\"\"Simple generator to produce MNIST images.\n",
    "    \n",
    "    Args:\n",
    "        noise: A single Tensor representing noise.\n",
    "        weight_decay: The value of the l2 weight decay.\n",
    "        is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n",
    "            norm uses the exponential moving average collected from population \n",
    "            statistics.\n",
    "    \n",
    "    Returns:\n",
    "        A generated image in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    with framework.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)),\\\n",
    "    framework.arg_scope([layers.batch_norm], is_training=is_training,\n",
    "                        zero_debias_moving_mean=True):\n",
    "        net = layers.fully_connected(noise, 1024)\n",
    "        net = layers.fully_connected(net, 7 * 7 * 256)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 256])\n",
    "        net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "        # Make sure that generator output is in the same range as `inputs`\n",
    "        # ie [-1, 1].\n",
    "        net = layers.conv2d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_fn(img, unused_conditioning, weight_decay=2.5e-5,\n",
    "                     is_training=True):\n",
    "    \"\"\"Discriminator network on MNIST digits.\n",
    "    \n",
    "    Args:\n",
    "        img: Real or generated MNIST digits. Should be in the range [-1, 1].\n",
    "        unused_conditioning: The TFGAN API can help with conditional GANs, which\n",
    "            would require extra `condition` information to both the generator and the\n",
    "            discriminator. Since this example is not conditional, we do not use this\n",
    "            argument.\n",
    "        weight_decay: The L2 weight decay.\n",
    "        is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n",
    "            norm uses the exponential moving average collected from population \n",
    "            statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Logits for the probability that the image is real.\n",
    "    \"\"\"\n",
    "    with framework.arg_scope(\n",
    "        [layers.conv2d, layers.fully_connected],\n",
    "        activation_fn=leaky_relu, normalizer_fn=None,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        net = layers.flatten(net)\n",
    "        with framework.arg_scope([layers.batch_norm], is_training=is_training):\n",
    "            net = layers.fully_connected(net, 1024, normalizer_fn=layers.batch_norm)\n",
    "        return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANModel Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\welcome\\envs\\dcgan\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\welcome\\envs\\dcgan\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    }
   ],
   "source": [
    "noise_dims = 64\n",
    "gan_model = tfgan.gan_model(\n",
    "    generator_fn,\n",
    "    discriminator_fn,\n",
    "    real_data=real_images,\n",
    "    generator_inputs=tf.random_normal([batch_size, noise_dims]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\welcome\\envs\\dcgan\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "vanilla loss generator loss: -0.753102\n",
      "vanilla loss discriminator loss: 1.535728\n",
      "improved wgan loss generator loss: -0.310039\n",
      "improved wgan loss discriminator loss: 0.074112\n",
      "custom loss generator loss: -0.577840\n",
      "custom loss discriminator loss: 0.076381\n"
     ]
    }
   ],
   "source": [
    "# We can use the minimax loss from the original paper.\n",
    "vanilla_gan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=tfgan.losses.minimax_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss)\n",
    "\n",
    "# We can use the Wasserstein loss (https://arxiv.org/abs/1701.07875) with the \n",
    "# gradient penalty from the improved Wasserstein loss paper \n",
    "# (https://arxiv.org/abs/1704.00028).\n",
    "improved_wgan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    # We make the loss explicit for demonstration, even though the default is \n",
    "    # Wasserstein loss.\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    gradient_penalty_weight=1.0)\n",
    "\n",
    "# We can also define custom losses to use with the rest of the TFGAN framework.\n",
    "def silly_custom_generator_loss(gan_model, add_summaries=False):\n",
    "    return tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
    "def silly_custom_discriminator_loss(gan_model, add_summaries=False):\n",
    "    return (tf.reduce_mean(gan_model.discriminator_gen_outputs) -\n",
    "            tf.reduce_mean(gan_model.discriminator_real_outputs))\n",
    "custom_gan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=silly_custom_generator_loss,\n",
    "    discriminator_loss_fn=silly_custom_discriminator_loss)\n",
    "\n",
    "# Sanity check that we can evaluate our losses.\n",
    "for gan_loss, name in [(vanilla_gan_loss, 'vanilla loss'), \n",
    "                       (improved_wgan_loss, 'improved wgan loss'), \n",
    "                       (custom_gan_loss, 'custom loss')]:\n",
    "    evaluate_tfgan_loss(gan_loss, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS\n",
      "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS\n",
      "WARNING:tensorflow:From c:\\users\\welcome\\envs\\dcgan\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(0.001, beta1=0.5)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5)\n",
    "gan_train_ops = tfgan.gan_train_ops(\n",
    "    gan_model,\n",
    "    improved_wgan_loss,\n",
    "    generator_optimizer,\n",
    "    discriminator_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "loss_values, mnist_scores, frechet_distances  = [], [], []\n",
    "\n",
    "with tf.train.SingularMonitoredSession() as sess:\n",
    "    start_time = time.time()\n",
    "    for i in range(1601):\n",
    "        cur_loss, _ = train_step_fn(\n",
    "            sess, gan_train_ops, global_step, train_step_kwargs={})\n",
    "        loss_values.append((i, cur_loss))\n",
    "        if i % 200 == 0:\n",
    "            mnist_score, f_distance, digits_np = sess.run(\n",
    "                [eval_score, frechet_distance, generated_data_to_visualize])\n",
    "            mnist_scores.append((i, mnist_score))\n",
    "            frechet_distances.append((i, f_distance))\n",
    "            print('Current loss: %f' % cur_loss)\n",
    "            print('Current MNIST score: %f' % mnist_scores[-1][1])\n",
    "            print('Current Frechet distance: %f' % frechet_distances[-1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
