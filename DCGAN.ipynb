{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# Scaling the range of the image to [-1, 1]\n",
    "# Because we are using tanh as the activation function in the last layer of the generator\n",
    "# and tanh restricts the weights in the range [-1, 1]\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W', dtype=tf.float32, shape=shape, initializer=initer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b', dtype=tf.float32, initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_out: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape()[1]\n",
    "        W = weight_variable(shape=[in_dim, num_units])\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_units])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.matmul(x, W)\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PRelu (parametarized leaky relu to correctly avoid dying Relu problem )  and Leaky Relu problem where negetive data only provide small amount of slope for learning parameter so that network itself familirize the required slope paramter for negative input value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prelu(inpt, name):\n",
    "    \"\"\"\n",
    "    @inpt: input from previous layer\n",
    "    @name: layer name\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        i = int(inpt.get_shape()[-1])\n",
    "        alpha = tf.get_variable('alpha', shape=(i,))\n",
    "        output = tf.nn.relu(inpt) + tf.multiply(alpha, -tf.nn.relu(-inpt))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, num_filters, filter_size, stride, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a 2D convolution layer\n",
    "    :param x: input from previous layer\n",
    "    :param filter_size: size of each filter\n",
    "    :param num_filters: number of filters (or output feature maps)\n",
    "    :param stride: filter stride\n",
    "    :param name: layer name\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        num_in_channel = x.get_shape().as_list()[-1]\n",
    "        shape = [filter_size, filter_size, num_in_channel, num_filters]\n",
    "        W = weight_variable(shape=shape)\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_filters])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "#         Using non - linearity RELU\n",
    "            return tf.nn.relu(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    \"\"\"\n",
    "    Flattens the output of the convolutional layer to be fed into fully-connected layer\n",
    "    :param layer: input array\n",
    "    :return: flattened array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Flatten_layer'):\n",
    "        layer_shape = layer.get_shape()\n",
    "        num_features = layer_shape[1:4].num_elements()\n",
    "        layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, keep_prob,name):\n",
    "    \"\"\"Create a dropout layer.\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001  # The optimization initial learning rate\n",
    "epochs = 20  # Total number of training epochs\n",
    "batch_size = 128  # Training batch size\n",
    "display_freq = 50  # Frequency of displaying the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name='Y')\n",
    "noise = tf.placeholder(tf.float32,shape=[None, 100], name='Noise')\n",
    "phase = tf.placeholder(tf.bool, name='phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\welcome\\envs\\dcgan\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Generator network\n",
    "g_net = fc_layer(noise, 1024, name='g_fc1',use_relu=False)\n",
    "g_net = Prelu(g_net, name='g_prelu_1')\n",
    "\n",
    "g_net = layers.batch_norm(g_net, is_training=phase)\n",
    "\n",
    "g_net = fc_layer(noise,7 * 7 * 256, name='g_fc2', use_relu=False)\n",
    "g_net = Prelu(g_net, name='g_prelu_2')\n",
    "#     Post activation Batch Normalization\n",
    "g_net = layers.batch_norm(g_net, is_training=phase)\n",
    "\n",
    "g_net = tf.reshape(g_net, [-1, 7, 7, 256])\n",
    "g_net = layers.conv2d_transpose(g_net, 64, [4, 4], stride=2)\n",
    "g_net = layers.conv2d_transpose(g_net, 32, [4, 4], stride=2)\n",
    "# Make sure that generator output is in the same range as `inputs`\n",
    "# ie [-1, 1].\n",
    "g_output = layers.conv2d(g_net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ef4a5eaa4576>:3: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Discriminator network\n",
    "d_net = conv_layer(x, 64, 5, stride = 2, name='d_conv_1',use_relu=False)\n",
    "d_net = Prelu(d_net, name='d_prelu_1')\n",
    "\n",
    "d_net = dropout(d_net, 0.3, name='drop_1')\n",
    "\n",
    "d_net = conv_layer(d_net, 128, 5, stride = 2, name='d_conv_2',use_relu=False)\n",
    "d_net = Prelu(d_net, name='d_prelu_2')\n",
    "\n",
    "d_net = dropout(d_net, 0.3, name='drop_2')\n",
    "\n",
    "d_net = flatten_layer(d_net)\n",
    "d_net = fc_layer(d_net,1,name='output_discriminator',use_relu=False)\n",
    "d_output= tf.nn.sigmoid(d_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('discriminator'):\n",
    "    with tf.variable_scope('Train'):\n",
    "        with tf.variable_scope('Loss'):\n",
    "            d_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=d_output), name='loss')\n",
    "        tf.summary.scalar('loss', d_loss)\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            d_optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam-op').minimize(d_loss)\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            d_correct_prediction = tf.equal(tf.argmax(d_output, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "            d_accuracy = tf.reduce_mean(tf.cast(d_correct_prediction, tf.float32), name='accuracy')\n",
    "        tf.summary.scalar('accuracy', d_accuracy)\n",
    "        with tf.variable_scope('Prediction'):\n",
    "#             d_cls_prediction = tf.argmax(d_output, axis=1, name='predictions')\n",
    "            d_cls_prediction = d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('generator'):\n",
    "    with tf.variable_scope('Train'):\n",
    "        with tf.variable_scope('Loss'):\n",
    "            g_loss = d_loss\n",
    "        tf.summary.scalar('loss', g_loss)\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                g_optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam-op').minimize(g_loss)\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            g_correct_prediction = tf.equal(tf.argmax(g_output, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "            g_accuracy = tf.reduce_mean(tf.cast(g_correct_prediction, tf.float32), name='accuracy')\n",
    "        tf.summary.scalar('accuracy', g_accuracy)\n",
    "        with tf.variable_scope('Prediction'):\n",
    "#             g_cls_prediction = tf.argmax(g_output, axis=1, name='predictions')\n",
    "            g_cls_prediction = g_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Merge all summaries\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :, :, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(init)\n",
    "global_step = 0\n",
    "summary_writer = tf.summary.FileWriter('./logdir', sess.graph)\n",
    "# Number of training iterations in each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training network visualizing the network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(preds):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for i in range(preds.shape[0]):\n",
    "        plt.subplot(10, 10, i+1)\n",
    "        plt.imshow(preds[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tr_iter = int(Y_train.shape[0]/ batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    x_train, y_train = randomize(x_train, y_train)\n",
    "    for iteration in range(num_tr_iter):\n",
    "        global_step += 1\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        image_batch ,_ = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # Input for the generator\n",
    "        noise_input = np.random.rand(batch_size, 100)\n",
    "        \n",
    "        # these are the predicted images from the generator\n",
    "        predictions = sess.run(g_cls_prediction,feed_dict={noise: noise_input,phase:0})\n",
    "        \n",
    "        # the discriminator takes in the real images and the generated images\n",
    "        X = np.concatenate([predictions, image_batch])\n",
    "\n",
    "        # labels for the discriminator\n",
    "        y_discriminator = [0]*batch_size + [1]*batch_size\n",
    "        \n",
    "        # Let's train the discriminator\n",
    "        sess.run(d_optimizer, feed_dict={x:X,y:y_discriminator})\n",
    "        \n",
    "        # Let's train the generator\n",
    "        noise_input = np.random.rand(batch_size, 100)\n",
    "        y_generator = [1]*batch_size\n",
    "        generate = sess.run(g_cls_prediction,feed_dict={noise: noise_input,phase:0})\n",
    "        sess.run(g_optimizer, feed_dict={x:X,y:y_discriminator})\n",
    "        \n",
    "#       lets look up new trained prediction of generator\n",
    "    \n",
    "    print(f\"Visualizing the  Generator Prediction at epoch: {epoch}\")\n",
    "    plot_output(sess.run(g_cls_prediction,feed_dict={noise: np.random.rand(batch_size, 100),phase:0}))\n",
    "    save_path = saver.save(sess, \"model.ckpt\"+\"_\"+str(epoch))\n",
    "    print(\"Model saved in path: %s\" % save_path+'_'+str(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
